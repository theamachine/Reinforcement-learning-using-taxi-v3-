{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu5AVY3UXhYF"
      },
      "source": [
        "# Install the main package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idwxBxG0EXVC",
        "outputId": "5f0f3959-ecb1-4162-8529-98ba91ab1e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.22.4)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKyrKhTBXvPs"
      },
      "source": [
        "# Importing main Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnryhY8UEmTH"
      },
      "outputs": [],
      "source": [
        "from numpy import array\n",
        "from math import inf\n",
        "from numpy.linalg import norm\n",
        "import gym\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMPfuuHBX4E4"
      },
      "source": [
        "# Defining environment function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aer2Cn5oEyAq"
      },
      "outputs": [],
      "source": [
        "def render(env):\n",
        "  env = gym.make(env)\n",
        "  env.render()\n",
        "  env.reset() # reset environment to a new, random state\n",
        "  print(\"Action Space {}\".format(env.action_space))\n",
        "  print(\"State Space {}\".format(env.observation_space))\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8_NkgJBpGYo",
        "outputId": "e0ca14a3-dff2-40c2-9507-77a8fc0c76aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ],
      "source": [
        "env = render('Taxi-v3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkhxD2AQE63V",
        "outputId": "a6524f7f-63eb-4ae1-9388-13f7226543e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = env.encode(3, 1, 2, 0) \n",
        "print(\"State:\", state)\n",
        "env.s = state\n",
        "env.render()\n",
        "env.P[328]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLMKFJshE7H8"
      },
      "outputs": [],
      "source": [
        "env.s = 328 \n",
        "def incorrect_time(env):\n",
        "  epochs = 0\n",
        "  penalties = 0\n",
        "  reward = 0\n",
        "  frames = []\n",
        "  done = False\n",
        "  while not done:\n",
        "      action = env.action_space.sample()\n",
        "      state, reward, done, info = env.step(action)\n",
        "      if reward == -10:\n",
        "          penalties += 1\n",
        "      frames.append({\n",
        "          'frame': env.render(mode='ansi'),\n",
        "          'state': state,\n",
        "          'action': action,\n",
        "          'reward': reward\n",
        "          }\n",
        "      )\n",
        "      epochs += 1\n",
        "  print(\"Timesteps taken: {}\".format(epochs))\n",
        "  print(\"Penalties incurred: {}\".format(penalties))\n",
        "  return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0wuKd3QpxSq",
        "outputId": "993b1c48-9a43-46cb-d73b-7397d0031aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timesteps taken: 200\n",
            "Penalties incurred: 72\n"
          ]
        }
      ],
      "source": [
        "frames = incorrect_time(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qrsF3XnYCsg"
      },
      "source": [
        "# Defining a function to show frames\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwcTHMI-E7LJ",
        "outputId": "6b4986e9-4122-4797-e79c-c76fce619d27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "Timestep: 200\n",
            "State: 93\n",
            "Action: 1\n",
            "Reward: -1\n"
          ]
        }
      ],
      "source": [
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "print_frames(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK5I8AShYGPw"
      },
      "source": [
        "# Defining the main function for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POl9-ygBFkG7"
      },
      "outputs": [],
      "source": [
        "def training(env):\n",
        "  q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "  # Hyperparameters\n",
        "  alpha = 0.1\n",
        "  gamma = 0.6\n",
        "  epsilon = 0.1\n",
        "\n",
        "  # For plotting metrics\n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "  table = []\n",
        "  for i in range(1, 100001):\n",
        "      state = env.reset()\n",
        "\n",
        "      epochs, penalties, reward, = 0, 0, 0\n",
        "      done = False\n",
        "      \n",
        "      while not done:\n",
        "          if random.uniform(0, 1) < epsilon:\n",
        "              action = env.action_space.sample() # Explore action space\n",
        "          else:\n",
        "              action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "          next_state, reward, done, info = env.step(action) \n",
        "          \n",
        "          old_value = q_table[state, action]\n",
        "          next_max = np.max(q_table[next_state])\n",
        "          \n",
        "          new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "          q_table[state, action] = new_value\n",
        "          table.append(q_table[state, action])\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          state = next_state\n",
        "          epochs += 1\n",
        "          \n",
        "      if i % 100 == 0:\n",
        "          clear_output(wait=True)\n",
        "          print(f\"Episode: {i}\")\n",
        "\n",
        "  print(\"Training finished.\\n\")\n",
        "  return q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhSjgZCmqg3y",
        "outputId": "6e146362-c11d-4258-c75d-12f479cbd4d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ -2.41837066,  -2.3639511 ,  -2.41837065,  -2.36395106,\n",
              "         -2.27325184, -11.36395098],\n",
              "       [ -1.87014398,  -1.45024005,  -1.870144  ,  -1.45024004,\n",
              "         -0.7504    , -10.45023897],\n",
              "       ...,\n",
              "       [ -1.03958441,   0.41599995,  -1.01106646,  -1.27151752,\n",
              "         -4.6641925 ,  -4.75285588],\n",
              "       [ -2.14527308,  -2.1220628 ,  -2.14621956,  -2.12206152,\n",
              "         -6.77056354,  -4.46843968],\n",
              "       [  2.82610541,   1.46325204,   2.51976598,  11.        ,\n",
              "         -2.768596  ,  -1.45600185]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_table = training(env)\n",
        "q_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1OJy3CgYNTI"
      },
      "source": [
        "# Evaluation function after training\n",
        "\n",
        "\n",
        "> calculating time steps and penalties\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7vy6xoNFz1g"
      },
      "outputs": [],
      "source": [
        "def apply_evaluate(q_table,env):\n",
        "  total_epochs, total_penalties = 0, 0\n",
        "  episodes = 100\n",
        "\n",
        "  for _ in range(episodes):\n",
        "      state = env.reset()\n",
        "      epochs, penalties, reward = 0, 0, 0\n",
        "      \n",
        "      done = False\n",
        "      \n",
        "      while not done:\n",
        "          action = np.argmax(q_table[state])\n",
        "          state, reward, done, info = env.step(action)\n",
        "\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          epochs += 1\n",
        "\n",
        "      total_penalties += penalties\n",
        "      total_epochs += epochs\n",
        "\n",
        "  \n",
        "  print(f\"Results after {episodes} episodes:\")\n",
        "  print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "  print(f\"Average penalties per episode: {total_penalties / episodes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbrM93I2pzUa",
        "outputId": "1613c86f-aba1-49db-e9b0-4a3406995b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.74\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ],
      "source": [
        "apply_evaluate(q_table,env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4ecC6zUwD7L",
        "outputId": "54274264-74f7-4481-d061-b7cecc78ba1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Action Space Discrete(4)\n",
            "State Space Discrete(16)\n"
          ]
        }
      ],
      "source": [
        "env_b = render('FrozenLake-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHamkIzjwEEC",
        "outputId": "b9148e93-ded5-448c-f005-dd71b8eb17c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timesteps taken: 5\n",
            "Penalties incurred: 0\n"
          ]
        }
      ],
      "source": [
        "frames_2 = incorrect_time(env_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Aski1UbyFWr",
        "outputId": "b3673a4a-fbbc-42bb-842b-99e3d53afb7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Timestep: 5\n",
            "State: 5\n",
            "Action: 1\n",
            "Reward: 0.0\n"
          ]
        }
      ],
      "source": [
        "print_frames(frames_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahvg1HNHwEJW",
        "outputId": "e93b682a-1d4f-4f2c-c494-fabe9ee4bacb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "q_table_b = training(env_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFF5McJZxR-r",
        "outputId": "76626b3c-983f-4c5e-ad7d-796dbd616828"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[6.45811256e-04, 1.39009412e-03, 8.01426734e-04, 6.65151561e-04],\n",
              "       [5.98032581e-04, 1.26957527e-03, 7.23996781e-04, 1.83415759e-03],\n",
              "       [5.30691661e-03, 2.26716351e-03, 3.47446943e-03, 1.43043957e-03],\n",
              "       [8.41343080e-04, 1.19459329e-03, 7.67681286e-04, 8.37178620e-04],\n",
              "       [1.47796868e-03, 2.50911636e-03, 1.59690554e-03, 7.78437162e-04],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [1.35877142e-02, 9.08746288e-03, 7.54278803e-03, 1.61139610e-03],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [1.81870935e-03, 7.65187898e-03, 8.20222094e-03, 4.93953252e-03],\n",
              "       [1.74396077e-02, 4.16208375e-02, 2.94600915e-02, 2.21194719e-02],\n",
              "       [3.08081957e-02, 5.03243636e-02, 9.04127706e-02, 9.77073758e-03],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [3.99172575e-02, 7.39256117e-02, 7.59393940e-02, 6.23507157e-02],\n",
              "       [1.40061568e-01, 6.26648006e-01, 2.04299208e-01, 2.18247530e-01],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_table_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da03XLQKwEMB",
        "outputId": "821f87c4-639d-43bf-8daa-9608e8c74e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 8.88\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ],
      "source": [
        "apply_evaluate(q_table_b,env_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4YknP6Uzv2j",
        "outputId": "1487e7ea-0326-4af5-8b45-160f56de1b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Action Space Discrete(4)\n",
            "State Space Discrete(48)\n"
          ]
        }
      ],
      "source": [
        "env_c = render('CliffWalking-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKMNmB4jzv7j",
        "outputId": "f235f3fe-fd51-4daa-d26c-6674a5c3995a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timesteps taken: 2556\n",
            "Penalties incurred: 0\n"
          ]
        }
      ],
      "source": [
        "frames_3 = incorrect_time(env_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyzQRiVHzwDL",
        "outputId": "8d7c3fd3-45d6-4e44-e3c8-f72f1b7dd5f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "q_table_c = training(env_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBfIpZaIzwFM",
        "outputId": "3af64503-d500-44f4-a11f-651f869d2c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.0\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ],
      "source": [
        "apply_evaluate(q_table_c,env_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N-uMg_HYUN9"
      },
      "source": [
        "# Hyperparameter tuning function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CDvpxL3zwJE"
      },
      "outputs": [],
      "source": [
        "def parameters_deg(env, alpha, gamma, epsilon):\n",
        "  q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "  # Hyperparameters\n",
        "  alpha = alpha\n",
        "  gamma = gamma\n",
        "  epsilon = epsilon\n",
        "\n",
        "  # For plotting metrics\n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "  table = []\n",
        "  for i in range(1, 100001):\n",
        "      state = env.reset()\n",
        "\n",
        "      epochs, penalties, reward, = 0, 0, 0\n",
        "      done = False\n",
        "      \n",
        "      while not done:\n",
        "          if random.uniform(0, 1) < epsilon:\n",
        "              action = env.action_space.sample() # Explore action space\n",
        "          else:\n",
        "              action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "          next_state, reward, done, info = env.step(action) \n",
        "          \n",
        "          first = q_table[state, action]\n",
        "          next_max = np.max(q_table[next_state])\n",
        "          \n",
        "          new_value = (1 - alpha) * first + alpha * (reward + gamma * next_max)\n",
        "          q_table[state, action] = new_value\n",
        "          table.append(q_table[state, action])\n",
        "          alpha = alpha - (0.0001*alpha)\n",
        "          gamma = gamma - (0.0001*gamma)\n",
        "          epsilon = epsilon - (0.0001*epsilon)\n",
        "          if alpha<=0:\n",
        "            alpha = 0.1\n",
        "          if gamma<=0:\n",
        "            gamma = 0.6\n",
        "          if epsilon<= 0:\n",
        "            epsilon = 0.1\n",
        "\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          state = next_state\n",
        "          epochs += 1\n",
        "          \n",
        "      if i % 100 == 0:\n",
        "          clear_output(wait=True)\n",
        "          print(f\"Episode: {i}\")\n",
        "\n",
        "  print(\"Training finished.\\n\")\n",
        "  return q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmRZX2xlzwLl",
        "outputId": "d7a80fe9-3331-447e-9a0a-bacb24ea30bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ],
      "source": [
        "env_hyper = render('Taxi-v3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njfiJXfqzwOS",
        "outputId": "cadcb43e-9d29-4fc5-9d6b-cc30aef007e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timesteps taken: 200\n",
            "Penalties incurred: 77\n"
          ]
        }
      ],
      "source": [
        "frames = incorrect_time(env_hyper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLFc4PguzwQp",
        "outputId": "7ed1d61c-261b-4be3-b70e-2c7b4ac3ef96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "q_table_d = parameters_deg(env_hyper,0.2,0.2,0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVyZa4NVh-d8",
        "outputId": "90837f52-2d20-45f6-a010-ad6397cef0dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 200.0\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ],
      "source": [
        "apply_evaluate(q_table_d,env_hyper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gxrrrAbYZuV"
      },
      "source": [
        "# Evaluation function for grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPGLnNIVzwTa"
      },
      "outputs": [],
      "source": [
        "def grid_search_evaluation(q_table,env):\n",
        "  total_epochs, total_penalties = 0, 0\n",
        "  episodes = 100\n",
        "\n",
        "  for _ in range(episodes):\n",
        "      state = env.reset()\n",
        "      epochs, penalties, reward = 0, 0, 0\n",
        "      \n",
        "      done = False\n",
        "      \n",
        "      while not done:\n",
        "          action = np.argmax(q_table[state])\n",
        "          state, reward, done, info = env.step(action)\n",
        "\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          epochs += 1\n",
        "\n",
        "      total_penalties += penalties\n",
        "      total_epochs += epochs\n",
        "\n",
        "  average_timesteps = total_epochs / episodes\n",
        "  average_penalties = total_penalties / episodes\n",
        "  print(f\"Results after {episodes} episodes:\")\n",
        "  print(f\"Average timesteps per episode: {average_timesteps}\")\n",
        "  print(f\"Average penalties per episode: {average_penalties}\")\n",
        "  return average_timesteps,average_penalties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_oene6LYv5U"
      },
      "source": [
        "# Training for grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY5LeikW_NFK"
      },
      "outputs": [],
      "source": [
        "def training_grid_search(env,alpha,gamma,epsilon):\n",
        "  q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "  \n",
        "  # The Hyperparameters:\n",
        "  alpha = alpha\n",
        "  gamma = gamma\n",
        "  epsilon = epsilon\n",
        "\n",
        " \n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "  table = []\n",
        "  for i in range(1, 100001):\n",
        "      state = env.reset()\n",
        "\n",
        "      epochs, penalties, reward, = 0, 0, 0\n",
        "      done = False\n",
        "      \n",
        "      while not done:\n",
        "          if random.uniform(0, 1) < epsilon:\n",
        "              action = env.action_space.sample() # Explore action space\n",
        "          else:\n",
        "              action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "          next_state, reward, done, info = env.step(action) \n",
        "          \n",
        "          old_value = q_table[state, action]\n",
        "          next_max = np.max(q_table[next_state])\n",
        "          \n",
        "          new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "          q_table[state, action] = new_value\n",
        "          table.append(q_table[state, action])\n",
        "          if reward == -10:\n",
        "              penalties += 1\n",
        "\n",
        "          state = next_state\n",
        "          epochs += 1\n",
        "          \n",
        "      if i % 100 == 0:\n",
        "          clear_output(wait=True)\n",
        "          print(f\"Episode: {i}\")\n",
        "\n",
        "  print(\"Training finished.\\n\")\n",
        "  return q_table,alpha,gamma,epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5mWdUCqY0iE"
      },
      "source": [
        "# Grid search Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvXWEa_DgRaM"
      },
      "outputs": [],
      "source": [
        "def grid_search(parm,env):\n",
        "  time_steps = 10000\n",
        "  penalties = 10000\n",
        "  parameter = parm\n",
        "  for i in parameter['alpha']:\n",
        "    for j in parameter['gamma']:\n",
        "      for k in parameter['epsilon']:\n",
        "        q_table,alpha,gamma,epsilon = training_grid_search(env,alpha=i,gamma=j,epsilon=k)\n",
        "        average_timesteps,average_penalties = grid_search_evaluation(q_table,env)\n",
        "        if average_timesteps<= time_steps:\n",
        "          if average_penalties <= penalties:\n",
        "            time_steps = average_timesteps\n",
        "            penalties = average_penalties\n",
        "            chosen_parameters = {'alpha':alpha,'gamma':gamma,'epsilon':epsilon,'Time':average_timesteps,'penalties':average_penalties}\n",
        "  return chosen_parameters        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObxlpKhUholj",
        "outputId": "e2dcd92e-3a24-42e0-8a4e-5152a49d53c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.73\n",
            "Average penalties per episode: 0.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Time': 12.66, 'alpha': 0.2, 'epsilon': 0.3, 'gamma': 0.1, 'penalties': 0.0}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paramter = {'alpha':[0.1,0.2,0.3],'gamma':[0.1,0.2,0.3],'epsilon':[0.1,0.2,0.3]}\n",
        "grid_search(paramter,env)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Reinforcement_Learning_Assignment (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
